2024-12-14 10:43:08,187 - __main__ - INFO - Initialized Hugging Face Inference Client with model: meta-llama/Llama-3.2-1B
2024-12-14 10:43:08,191 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.126.13:5000
2024-12-14 10:43:08,191 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-12-14 10:43:18,917 - __main__ - INFO - User input:  gender male, Senior_Citizen no, Is_Married yes, Dependents yes, tenure 10, Dual yes, Contract month-to-month, Paperless_Billing yes, Payment_Method electronic check, Monthly_Charges 100, Online_Security yes, Online_Backup yes, Device_Protection no, Streaming_TV yes, Streaming_Movies no, Internet_Service yes, Tech_Support yes
2024-12-14 10:43:18,925 - __main__ - INFO - Input successfully parsed and converted to dataframe.
2024-12-14 10:43:18,989 - __main__ - INFO - Predicted churn probability: 0.1769958736004606
2024-12-14 10:43:18,989 - __main__ - INFO - Generated prompt: you are a helpful assistant that help the marketing team predict churn probability
        for customers. you are the best chatbot assistant in the world.output the churn equals Low Risk and output the probability equals 0.1769958736004606
2024-12-14 10:43:20,475 - __main__ - ERROR - Error during response generation: Input validation error: `temperature` must be strictly positive
2024-12-14 10:43:20,535 - __main__ - ERROR - Traceback (most recent call last):
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/utils/_http.py", line 406, in hf_raise_for_status
    response.raise_for_status()
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/inference/_client.py", line 2302, in text_generation
    bytes_output = self.post(json=payload, model=model, task="text-generation", stream=stream)  # type: ignore
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/inference/_client.py", line 296, in post
    hf_raise_for_status(response)
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/utils/_http.py", line 477, in hf_raise_for_status
    raise _format(HfHubHTTPError, str(e), response) from e
huggingface_hub.errors.HfHubHTTPError: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B (Request ID: Hank9EtEw9ctKIB0sHneX)

Input validation error: `temperature` must be strictly positive
Make sure 'text-generation' task is supported by the model.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "ll.py", line 279, in generate_response
    llm_response = self.llm.text_generation(prompt, max_new_tokens=500, temperature=0.0,repetition_penalty=5.0)
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/inference/_client.py", line 2332, in text_generation
    raise_text_generation_error(e)
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/inference/_common.py", line 463, in raise_text_generation_error
    raise exception from http_error
huggingface_hub.errors.ValidationError: Input validation error: `temperature` must be strictly positive

2024-12-14 10:43:20,536 - ll - ERROR - Exception on /api/v1/response [POST]
Traceback (most recent call last):
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/utils/_http.py", line 406, in hf_raise_for_status
    response.raise_for_status()
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/inference/_client.py", line 2302, in text_generation
    bytes_output = self.post(json=payload, model=model, task="text-generation", stream=stream)  # type: ignore
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/inference/_client.py", line 296, in post
    hf_raise_for_status(response)
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/utils/_http.py", line 477, in hf_raise_for_status
    raise _format(HfHubHTTPError, str(e), response) from e
huggingface_hub.errors.HfHubHTTPError: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B (Request ID: Hank9EtEw9ctKIB0sHneX)

Input validation error: `temperature` must be strictly positive
Make sure 'text-generation' task is supported by the model.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "ll.py", line 279, in generate_response
    llm_response = self.llm.text_generation(prompt, max_new_tokens=500, temperature=0.0,repetition_penalty=5.0)
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/inference/_client.py", line 2332, in text_generation
    raise_text_generation_error(e)
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/inference/_common.py", line 463, in raise_text_generation_error
    raise exception from http_error
huggingface_hub.errors.ValidationError: Input validation error: `temperature` must be strictly positive

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "ll.py", line 317, in getResponse
    response = chatbot.generate_response(user_input)
  File "ll.py", line 285, in generate_response
    raise RuntimeError(f"Could not generate response: {e}")
RuntimeError: Could not generate response: Input validation error: `temperature` must be strictly positive
2024-12-14 10:43:20,540 - werkzeug - INFO - 192.168.126.25 - - [14/Dec/2024 10:43:20] "[35m[1mPOST /api/v1/response HTTP/1.1[0m" 500 -
2024-12-14 10:43:32,037 - __main__ - INFO - User input:  gender male, Senior_Citizen no, Is_Married yes, Dependents yes, tenure 10, Dual yes, Contract month-to-month, Paperless_Billing yes, Payment_Method electronic check, Monthly_Charges 100, Online_Security yes, Online_Backup yes, Device_Protection no, Streaming_TV yes, Streaming_Movies no, Internet_Service yes, Tech_Support yes
2024-12-14 10:43:32,042 - __main__ - INFO - Input successfully parsed and converted to dataframe.
2024-12-14 10:43:32,099 - __main__ - INFO - Predicted churn probability: 0.1769958736004606
2024-12-14 10:43:32,100 - __main__ - INFO - Generated prompt: you are a helpful assistant that help the marketing team predict churn probability
        for customers. you are the best chatbot assistant in the world.output the churn equals Low Risk and output the probability equals 0.1769958736004606
2024-12-14 10:43:32,413 - __main__ - ERROR - Error during response generation: Input validation error: `temperature` must be strictly positive
2024-12-14 10:43:32,414 - __main__ - ERROR - Traceback (most recent call last):
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/utils/_http.py", line 406, in hf_raise_for_status
    response.raise_for_status()
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/inference/_client.py", line 2302, in text_generation
    bytes_output = self.post(json=payload, model=model, task="text-generation", stream=stream)  # type: ignore
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/inference/_client.py", line 296, in post
    hf_raise_for_status(response)
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/utils/_http.py", line 477, in hf_raise_for_status
    raise _format(HfHubHTTPError, str(e), response) from e
huggingface_hub.errors.HfHubHTTPError: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B (Request ID: UyfJ52VSI3nsanU24HYE0)

Input validation error: `temperature` must be strictly positive
Make sure 'text-generation' task is supported by the model.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "ll.py", line 279, in generate_response
    llm_response = self.llm.text_generation(prompt, max_new_tokens=500, temperature=0.0,repetition_penalty=5.0)
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/inference/_client.py", line 2332, in text_generation
    raise_text_generation_error(e)
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/inference/_common.py", line 463, in raise_text_generation_error
    raise exception from http_error
huggingface_hub.errors.ValidationError: Input validation error: `temperature` must be strictly positive

2024-12-14 10:43:32,415 - ll - ERROR - Exception on /api/v1/response [POST]
Traceback (most recent call last):
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/utils/_http.py", line 406, in hf_raise_for_status
    response.raise_for_status()
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/inference/_client.py", line 2302, in text_generation
    bytes_output = self.post(json=payload, model=model, task="text-generation", stream=stream)  # type: ignore
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/inference/_client.py", line 296, in post
    hf_raise_for_status(response)
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/utils/_http.py", line 477, in hf_raise_for_status
    raise _format(HfHubHTTPError, str(e), response) from e
huggingface_hub.errors.HfHubHTTPError: 422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-1B (Request ID: UyfJ52VSI3nsanU24HYE0)

Input validation error: `temperature` must be strictly positive
Make sure 'text-generation' task is supported by the model.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "ll.py", line 279, in generate_response
    llm_response = self.llm.text_generation(prompt, max_new_tokens=500, temperature=0.0,repetition_penalty=5.0)
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/inference/_client.py", line 2332, in text_generation
    raise_text_generation_error(e)
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/huggingface_hub/inference/_common.py", line 463, in raise_text_generation_error
    raise exception from http_error
huggingface_hub.errors.ValidationError: Input validation error: `temperature` must be strictly positive

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/home/mazen/Desktop/new/etisalatEnv/lib/python3.8/site-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "ll.py", line 317, in getResponse
    response = chatbot.generate_response(user_input)
  File "ll.py", line 285, in generate_response
    raise RuntimeError(f"Could not generate response: {e}")
RuntimeError: Could not generate response: Input validation error: `temperature` must be strictly positive
2024-12-14 10:43:32,418 - werkzeug - INFO - 192.168.126.25 - - [14/Dec/2024 10:43:32] "[35m[1mPOST /api/v1/response HTTP/1.1[0m" 500 -
